{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgMzg52Xdyvy",
        "outputId": "842f8d73-d451-479d-b659-88d88a4615de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# This command installs a set of compatible library versions\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 pandas_ta vaderSentiment requests keras-tuner -q\n",
        "\n",
        "# This command will forcefully crash and restart the Colab kernel\n",
        "# to ensure the new libraries are loaded correctly.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_qfz6__eAJo",
        "outputId": "db6d2832-cedb-45f6-856c-9a7b09df9295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 29s]\n",
            "val_loss: 0.0027795617934316397\n",
            "\n",
            "Best val_loss So Far: 0.0027773799374699593\n",
            "Total elapsed time: 00h 04m 56s\n",
            "Search complete. Retraining the best model on the full dataset...\n",
            "Successfully trained and saved BEST model to best_model_pharma.keras\n",
            "Successfully saved BEST scaler to best_scaler_pharma.save\n",
            "Optimal hyperparameters: {'units_1': 96, 'dropout_1': 0.30000000000000004, 'units_2': 64, 'dropout_2': 0.2, 'units_3': 32, 'learning_rate': 0.001}\n",
            "\n",
            "--- All models have been tuned and trained successfully! ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import joblib\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas_ta as ta\n",
        "\n",
        "# --- 1. Sentiment Analysis Setup ---\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "NEWS_API_KEY = \"API_KEY\" # <-- IMPORTANT: Replace with your key\n",
        "\n",
        "def get_sentiment_for_daterange(stock_ticker, start_date, end_date):\n",
        "    if NEWS_API_KEY == \"YOUR_NEWS_API_KEY\":\n",
        "        return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "    query = stock_ticker.split('.')[0]\n",
        "    url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={NEWS_API_KEY}&language=en&from={start_date}&to={end_date}&sortBy=publishedAt&pageSize=100\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        articles = response.json().get('articles', [])\n",
        "        if not articles:\n",
        "            return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "        news_df = pd.DataFrame(articles)\n",
        "        news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt']).dt.date\n",
        "        news_df['sentiment'] = news_df['title'].apply(lambda title: analyzer.polarity_scores(title)['compound'] if title else 0)\n",
        "        daily_sentiment = news_df.groupby('publishedAt')['sentiment'].mean()\n",
        "        daily_sentiment.index = pd.to_datetime(daily_sentiment.index)\n",
        "        all_days = pd.date_range(start=start_date, end=end_date)\n",
        "        sentiment_series = daily_sentiment.reindex(all_days, fill_value=0.0)\n",
        "        sentiment_series.name = f'{stock_ticker}_Sentiment'\n",
        "        return sentiment_series\n",
        "    except Exception as e:\n",
        "        print(f\"  - Could not fetch news for {stock_ticker}: {e}\")\n",
        "        return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "\n",
        "# --- 2. KerasTuner Model Builder Function ---\n",
        "def build_model(hp, input_shape, num_outputs):\n",
        "    \"\"\"\n",
        "    This function defines the search space for the hyperparameter tuner.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Tune the number of units in the first LSTM layer\n",
        "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)\n",
        "    model.add(LSTM(units=hp_units_1, return_sequences=True, input_shape=input_shape))\n",
        "\n",
        "    # Tune the dropout rate\n",
        "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
        "    model.add(Dropout(hp_dropout_1))\n",
        "\n",
        "    # Tune the number of units in the second LSTM layer\n",
        "    hp_units_2 = hp.Int('units_2', min_value=32, max_value=128, step=32)\n",
        "    model.add(LSTM(units=hp_units_2, return_sequences=False))\n",
        "\n",
        "    hp_dropout_2 = hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)\n",
        "    model.add(Dropout(hp_dropout_2))\n",
        "\n",
        "    # Tune the number of units in the Dense layer\n",
        "    hp_units_3 = hp.Int('units_3', min_value=16, max_value=64, step=16)\n",
        "    model.add(Dense(units=hp_units_3, activation='relu'))\n",
        "\n",
        "    model.add(Dense(units=num_outputs))\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_and_train_final_model(config):\n",
        "    print(f\"\\n--- Starting FINAL HYPERPARAMETER TUNING for the {config['sector_name']} sector model ---\")\n",
        "\n",
        "    # --- 3. Load and Prepare Data (as before) ---\n",
        "    print(\"Loading and preparing data...\")\n",
        "    start_date = '2015-01-01'\n",
        "    end_date = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "    data = yf.download(config['feature_tickers'], start=start_date, end=end_date)\n",
        "    df = data['Close'].copy()\n",
        "    if config['rename_map']:\n",
        "        df.rename(columns=config['rename_map'], inplace=True)\n",
        "    df.ffill(inplace=True)\n",
        "\n",
        "    print(\"Calculating technical indicators...\")\n",
        "    for stock in config['target_stocks']:\n",
        "        df[f'{stock}_SMA_20'] = ta.sma(df[stock], length=20)\n",
        "        df[f'{stock}_RSI_14'] = ta.rsi(df[stock], length=14)\n",
        "\n",
        "    print(\"Fetching and adding sentiment scores...\")\n",
        "    for stock in config['target_stocks']:\n",
        "        sentiment_data = get_sentiment_for_daterange(stock, start_date, end_date)\n",
        "        df = df.join(sentiment_data)\n",
        "        time.sleep(1)\n",
        "\n",
        "    df.ffill(inplace=True)\n",
        "\n",
        "    for stock in config['target_stocks']:\n",
        "        df[f'{stock}_Return'] = df[stock].pct_change()\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    feature_cols = [col for col in df.columns if '_Return' not in col]\n",
        "    target_cols = [f'{stock}_Return' for stock in config['target_stocks']]\n",
        "\n",
        "    train_size = int(len(df) * 0.85)\n",
        "    train_data = df.iloc[:train_size]\n",
        "    val_data = df.iloc[train_size:] # Use the test set as a validation set for tuning\n",
        "\n",
        "    if train_data.empty or len(train_data) < 60 or val_data.empty or len(val_data) < 60:\n",
        "        print(f\"Skipping {config['sector_name']} due to insufficient data.\")\n",
        "        return\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler.fit(train_data)\n",
        "\n",
        "    # Prepare training data\n",
        "    scaled_train_data = scaler.transform(train_data)\n",
        "    scaled_train_df = pd.DataFrame(scaled_train_data, columns=df.columns, index=train_data.index)\n",
        "    time_step = 60\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(time_step, len(scaled_train_df)):\n",
        "        X_train.append(scaled_train_df[feature_cols].values[i-time_step:i, :])\n",
        "        y_train.append(scaled_train_df[target_cols].values[i, :])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "    # Prepare validation data\n",
        "    scaled_val_data = scaler.transform(val_data)\n",
        "    scaled_val_df = pd.DataFrame(scaled_val_data, columns=df.columns, index=val_data.index)\n",
        "    X_val, y_val = [], []\n",
        "    for i in range(time_step, len(scaled_val_df)):\n",
        "        X_val.append(scaled_val_df[feature_cols].values[i-time_step:i, :])\n",
        "        y_val.append(scaled_val_df[target_cols].values[i, :])\n",
        "    X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "    # --- 4. Run the Hyperparameter Search ---\n",
        "    print(\"Starting hyperparameter search...\")\n",
        "    tuner = kt.RandomSearch(\n",
        "        lambda hp: build_model(hp, input_shape=(X_train.shape[1], X_train.shape[2]), num_outputs=len(config['target_stocks'])),\n",
        "        objective='val_loss',\n",
        "        max_trials=10, # Number of different model combinations to test\n",
        "        executions_per_trial=1,\n",
        "        directory='keras_tuner_dir',\n",
        "        project_name=f\"{config['sector_name']}_tuning\"\n",
        "    )\n",
        "\n",
        "    tuner.search(X_train, y_train, epochs=25, validation_data=(X_val, y_val)) # Search with fewer epochs for speed\n",
        "\n",
        "    # --- 5. Get the Best Model and Retrain It ---\n",
        "    print(\"Search complete. Retraining the best model on the full dataset...\")\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Combine train and val data to retrain on the full dataset\n",
        "    full_X = np.concatenate((X_train, X_val))\n",
        "    full_y = np.concatenate((y_train, y_val))\n",
        "\n",
        "    best_model.fit(full_X, full_y, epochs=75, batch_size=32, verbose=0) # Retrain for more epochs\n",
        "\n",
        "    # --- 6. Save the Final, Optimized Artifacts ---\n",
        "    best_model.save(config['model_save_path'])\n",
        "    joblib.dump(scaler, config['scaler_save_path'])\n",
        "    print(f\"Successfully trained and saved BEST model to {config['model_save_path']}\")\n",
        "    print(f\"Successfully saved BEST scaler to {config['scaler_save_path']}\")\n",
        "    print(f\"Optimal hyperparameters: {best_hps.values}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    SECTOR_DEFINITIONS = [\n",
        "        {\n",
        "            'sector_name': 'IT',\n",
        "            'companies': ['TCS.NS', 'INFY.NS', 'WIPRO.NS', 'HCLTECH.NS'],\n",
        "            'indices': {'^CNXIT': 'Nifty_IT_Index', '^NSEI': 'Nifty_50_Index'}\n",
        "        },\n",
        "        {\n",
        "            'sector_name': 'Auto',\n",
        "            'companies': ['TATAMOTORS.NS', 'MARUTI.NS', 'M&M.NS', 'BAJAJ-AUTO.NS'],\n",
        "            'indices': {'^CNXAUTO': 'Nifty_Auto_Index'}\n",
        "        },\n",
        "        {\n",
        "            'sector_name': 'Banking',\n",
        "            'companies': ['HDFCBANK.NS', 'ICICIBANK.NS', 'SBIN.NS', 'KOTAKBANK.NS'],\n",
        "            'indices': {'^NSEBANK': 'Nifty_Bank_Index'}\n",
        "        },\n",
        "        {\n",
        "            'sector_name': 'FMCG',\n",
        "            'companies': ['HINDUNILVR.NS', 'ITC.NS', 'NESTLEIND.NS', 'BRITANNIA.NS'],\n",
        "            'indices': {'^CNXFMCG': 'Nifty_FMCG_Index'}\n",
        "        },\n",
        "        {\n",
        "            'sector_name': 'Pharma',\n",
        "            'companies': ['SUNPHARMA.NS', 'CIPLA.NS', 'DRREDDY.NS', 'DIVISLAB.NS'],\n",
        "            'indices': {'^CNXPHARMA': 'Nifty_Pharma_Index'}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for sector_config in SECTOR_DEFINITIONS:\n",
        "        sector_name_safe = sector_config['sector_name'].lower()\n",
        "        feature_tickers = sector_config['companies'] + list(sector_config['indices'].keys())\n",
        "\n",
        "        job_config = {\n",
        "            'sector_name': sector_config['sector_name'],\n",
        "            'target_stocks': sector_config['companies'],\n",
        "            'feature_tickers': feature_tickers,\n",
        "            'rename_map': sector_config['indices'],\n",
        "            'model_save_path': f'best_model_{sector_name_safe}.keras',\n",
        "            'scaler_save_path': f'best_scaler_{sector_name_safe}.save'\n",
        "        }\n",
        "\n",
        "        tune_and_train_final_model(job_config)\n",
        "\n",
        "    print(\"\\n--- All models have been tuned and trained successfully! ---\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
