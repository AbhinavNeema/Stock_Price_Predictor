{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-31KppP7zRy",
        "outputId": "8690edcb-99b2-4aa4-9e68-a45072a19a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# This command installs a set of compatible library versions\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 pandas_ta vaderSentiment requests keras-tuner -q\n",
        "\n",
        "# This command will forcefully crash and restart the Colab kernel\n",
        "# to ensure the new libraries are loaded correctly.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import joblib\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas_ta as ta\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "NEWS_API_KEY = \"f4720ca914da4e1eba03a3f520aa17f7\"\n",
        "\n",
        "def get_sentiment_for_daterange(stock_ticker, start_date, end_date):\n",
        "    if NEWS_API_KEY == \"YOUR_NEWS_API_KEY\":\n",
        "        return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "    query = stock_ticker.split('.')[0]\n",
        "    url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={NEWS_API_KEY}&language=en&from={start_date}&to={end_date}&sortBy=publishedAt&pageSize=100\"\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        articles = r.json().get('articles', [])\n",
        "        if not articles:\n",
        "            return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "        df = pd.DataFrame(articles)\n",
        "        df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.date\n",
        "        df['sentiment'] = df['title'].apply(lambda t: analyzer.polarity_scores(t)['compound'] if t else 0)\n",
        "        daily = df.groupby('publishedAt')['sentiment'].mean()\n",
        "        daily.index = pd.to_datetime(daily.index)\n",
        "        all_days = pd.date_range(start=start_date, end=end_date)\n",
        "        out = daily.reindex(all_days, fill_value=0.0)\n",
        "        out.name = f'{stock_ticker}_Sentiment'\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print(f\"couldn't fetch news for {stock_ticker}: {e}\")\n",
        "        return pd.Series(0.0, index=pd.date_range(start=start_date, end=end_date), name=f'{stock_ticker}_Sentiment')\n",
        "\n",
        "def build_model(hp, input_shape, num_outputs):\n",
        "    m = Sequential()\n",
        "    u1 = hp.Int('units_1', 32, 128, step=32)\n",
        "    m.add(LSTM(u1, return_sequences=True, input_shape=input_shape))\n",
        "    d1 = hp.Float('dropout_1', 0.2, 0.5, step=0.1)\n",
        "    m.add(Dropout(d1))\n",
        "    u2 = hp.Int('units_2', 32, 128, step=32)\n",
        "    m.add(LSTM(u2, return_sequences=False))\n",
        "    d2 = hp.Float('dropout_2', 0.2, 0.5, step=0.1)\n",
        "    m.add(Dropout(d2))\n",
        "    u3 = hp.Int('units_3', 16, 64, step=16)\n",
        "    m.add(Dense(u3, activation='relu'))\n",
        "    m.add(Dense(num_outputs))\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    m.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "    return m\n",
        "\n",
        "def tune_and_train_final_model(cfg):\n",
        "    print(f\"\\n>>> tuning {cfg['sector_name']} <<<\")\n",
        "    start_date = '2015-01-01'\n",
        "    end_date = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "    data = yf.download(cfg['feature_tickers'], start=start_date, end=end_date)\n",
        "    df = data['Close'].copy()\n",
        "    if cfg['rename_map']:\n",
        "        df.rename(columns=cfg['rename_map'], inplace=True)\n",
        "    df.ffill(inplace=True)\n",
        "    for s in cfg['target_stocks']:\n",
        "        df[f'{s}_SMA_20'] = ta.sma(df[s], length=20)\n",
        "        df[f'{s}_RSI_14'] = ta.rsi(df[s], length=14)\n",
        "    for s in cfg['target_stocks']:\n",
        "        sent = get_sentiment_for_daterange(s, start_date, end_date)\n",
        "        df = df.join(sent)\n",
        "        time.sleep(1)\n",
        "    df.ffill(inplace=True)\n",
        "    for s in cfg['target_stocks']:\n",
        "        df[f'{s}_Return'] = df[s].pct_change()\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    feat = [c for c in df.columns if '_Return' not in c]\n",
        "    tgt = [f'{s}_Return' for s in cfg['target_stocks']]\n",
        "\n",
        "    train_size = int(len(df) * 0.85)\n",
        "    train, val = df.iloc[:train_size], df.iloc[train_size:]\n",
        "    if train.empty or len(train) < 60 or val.empty or len(val) < 60:\n",
        "        print(f\"skip {cfg['sector_name']} (not enough data)\")\n",
        "        return\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train)\n",
        "\n",
        "    s_train = scaler.transform(train)\n",
        "    s_val = scaler.transform(val)\n",
        "    train_df = pd.DataFrame(s_train, columns=df.columns, index=train.index)\n",
        "    val_df = pd.DataFrame(s_val, columns=df.columns, index=val.index)\n",
        "\n",
        "    step = 60\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(step, len(train_df)):\n",
        "        X_train.append(train_df[feat].values[i-step:i, :])\n",
        "        y_train.append(train_df[tgt].values[i, :])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "    X_val, y_val = [], []\n",
        "    for i in range(step, len(val_df)):\n",
        "        X_val.append(val_df[feat].values[i-step:i, :])\n",
        "        y_val.append(val_df[tgt].values[i, :])\n",
        "    X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "    tuner = kt.RandomSearch(\n",
        "        lambda hp: build_model(hp, input_shape=(X_train.shape[1], X_train.shape[2]), num_outputs=len(cfg['target_stocks'])),\n",
        "        objective='val_loss',\n",
        "        max_trials=10,\n",
        "        executions_per_trial=1,\n",
        "        directory='keras_tuner_dir',\n",
        "        project_name=f\"{cfg['sector_name']}_tuning\"\n",
        "    )\n",
        "    tuner.search(X_train, y_train, epochs=25, validation_data=(X_val, y_val))\n",
        "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "    model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "    full_X = np.concatenate((X_train, X_val))\n",
        "    full_y = np.concatenate((y_train, y_val))\n",
        "    model.fit(full_X, full_y, epochs=75, batch_size=32, verbose=0)\n",
        "\n",
        "    model.save(cfg['model_save_path'])\n",
        "    joblib.dump(scaler, cfg['scaler_save_path'])\n",
        "    print(f\"{cfg['sector_name']} done! best params: {best_hp.values}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    SECTORS = [\n",
        "        {'sector_name': 'IT','companies': ['TCS.NS','INFY.NS','WIPRO.NS','HCLTECH.NS'],'indices': {'^CNXIT': 'Nifty_IT_Index','^NSEI': 'Nifty_50_Index'}},\n",
        "        {'sector_name': 'Auto','companies': ['TATAMOTORS.NS','MARUTI.NS','M&M.NS','BAJAJ-AUTO.NS'],'indices': {'^CNXAUTO': 'Nifty_Auto_Index'}},\n",
        "        {'sector_name': 'Banking','companies': ['HDFCBANK.NS','ICICIBANK.NS','SBIN.NS','KOTAKBANK.NS'],'indices': {'^NSEBANK': 'Nifty_Bank_Index'}},\n",
        "        {'sector_name': 'FMCG','companies': ['HINDUNILVR.NS','ITC.NS','NESTLEIND.NS','BRITANNIA.NS'],'indices': {'^CNXFMCG': 'Nifty_FMCG_Index'}},\n",
        "        {'sector_name': 'Pharma','companies': ['SUNPHARMA.NS','CIPLA.NS','DRREDDY.NS','DIVISLAB.NS'],'indices': {'^CNXPHARMA': 'Nifty_Pharma_Index'}}\n",
        "    ]\n",
        "\n",
        "    for s in SECTORS:\n",
        "        name = s['sector_name'].lower()\n",
        "        feats = s['companies'] + list(s['indices'].keys())\n",
        "        cfg = {\n",
        "            'sector_name': s['sector_name'],\n",
        "            'target_stocks': s['companies'],\n",
        "            'feature_tickers': feats,\n",
        "            'rename_map': s['indices'],\n",
        "            'model_save_path': f'best_model_{name}.keras',\n",
        "            'scaler_save_path': f'best_scaler_{name}.save'\n",
        "        }\n",
        "        tune_and_train_final_model(cfg)\n",
        "    print(\"\\nall done :)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnli0S0u758y",
        "outputId": "1b4f6c44-df87-4abf-d903-375d7b630b8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 28s]\n",
            "val_loss: 0.0029092878103256226\n",
            "\n",
            "Best val_loss So Far: 0.002766660414636135\n",
            "Total elapsed time: 00h 04m 31s\n",
            "Pharma done! best params: {'units_1': 64, 'dropout_1': 0.30000000000000004, 'units_2': 96, 'dropout_2': 0.30000000000000004, 'units_3': 16, 'learning_rate': 0.001}\n",
            "\n",
            "all done :)\n"
          ]
        }
      ]
    }
  ]
}